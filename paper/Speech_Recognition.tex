\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath}
% use geometry to set size to A4 and set margins around the document
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{hyperref} 
\usepackage{graphicx}
\graphicspath{ {paper/images/} }

\begin{document}

\title{Microsoft Azure Speech Recognition}
\author{Robin Vonk - 500775219 \\
		Michel Rummens - 500778934 \\}
\date{15/12/2019}

% Distance between the two text columns
\setlength{\columnsep}{30px}

% Set text in two columns
\twocolumn[
% Disable two columns for title page and abstract
\begin{@twocolumnfalse}
\begin{center}
    \maketitle
    \vspace*{-0.8cm}
    
    \rule{0.9\textwidth}{0.1mm} 
    
    \begin{abstract}
        \normalsize 
        Abstract
        TODO Add abstract here
        \vspace*{0.3cm}
    \end{abstract}
    
    \rule{0.9\textwidth}{0.1mm} 

\end{center}

\end{@twocolumnfalse}
]

\tableofcontents
\section{Introduction}
This paper will discuss the correctness and speed of the Microsoft Azure Speech to Text algorithm. This paper will not discuss the working nor any technical aspect of the algorithm. This paper will use a test set of audio fragments, run these tests through the algorithm and use the output to determine the strengths and weaknesses of the algorithm and calculate some values which can be used to compare the algorithm with other algorithms in the same field.

\subsection{Main question}
What is the real time factor, word error rate, word correct rate, recall, precision and F-score of the Microsoft speech to text algorithm with our test set?

\subsection{Hypothesis}
The speech to text algorithm of Microsoft is one of the best tested algorithm in this field\cite{Veton}. For this reason it is expected that this algorithm will do really well. It's results should little to not differ from original transcript of the audio files in any situation. \\ \\
Because of previous experiences with speech to text services, e.g. Siri \cite{Siri}, we expect the real time factor to be under one second for one sentence audio files.

\subsection{About Microsoft Speech to Text}
'The speech-to-text service defaults to using the Universal language model. This model was trained using Microsoft-owned data and is deployed in the cloud. It's optimal for conversational and dictation scenarios. When using speech-to-text for recognition and transcription in a unique environment, you can create and train custom acoustic, language, and pronunciation models. Customization is helpful for addressing ambient noise or industry-specific vocabulary.'\cite{Microsoft}

\section{Experiment}
All code and resources used for this experiment can be found on the Github page: \\
\url{https://github.com/obin1000/Micorsoft_Speech_Recognition_Test}

\subsection{Resources used}
\begin{itemize}
    \item Microsoft Azure Speech Recognition
    \item Python 3 numpy, sklearn
    \item Audio test set from fellow students
\end{itemize}


\subsection{Method}
The decision to use Python for this experiment was made because of its simple syntax and its ability to work with big numbers. A script was made which does the following:
\begin{itemize}
    \item Scan a directory for all audio files (ending on .wav)
    \item Load the transcriptions from a CSV file
    \item Match the audio files with the transcription based on file name and transcription tag
    \item Run the audio file through the speech to text algorithm
    \item Do calculation based on the results(Next chapter)
    \item Save the results to the results file
\end{itemize}

\subsection{Calculations}
The following formulas were used to compare the transcription of the audio with the output of the speech to text algorithm. These are standard formulas, this means that the results can be compared with the results of other speech to text algorithms. \\

    Real Time Factor: \\
    The time it takes for the algorithm to provide a response. Lower is better.
    \begin{center}
    $ RTF = Amount of recognized characters / time$ \\
    \end{center} 

    Word Error Rate (WER): \\
    For measuring WER you need:
    \begin{itemize}
        \item S = Substitutions:  word is replaced
        \item D = Deletions:  word is missed out
        \item I = Insertions:  word is added
        \item C = Corrects:  word matches
        \item N = Number of words in reference sentence
    \end{itemize}
    This should result in a value between 0 and 1, telling you how many of the words were wrongly recognized. Lower is better.
    \begin{center}
    $ WER = \frac{S + D + I}{N}$ \\
    \end{center} 
    
    Word Recognition Rate (WRR): \\
    This is the inverse of the WER. Should result in a value between 0 and 1, telling you how many of the words were correctly recognized. Higher is better.
    \begin{center}
    $ WRR = 1 - WER = \frac{C - I}{N}$ \\
    $ NBC =N - (S + D)$ \\
    \end{center} 
    
    Word Correct Rate (WCR): \\
    Should result in a value between 0 and 1, telling you how many of the words were correctly recognized. Higher is better.
    \begin{center}
    $ WCR = \frac{C}{N}$ \\
    \end{center} 
    
    Precision: \\
    Results in a number between 0 and 1, Answering: of all result found, how many of the are correct? Higher is better.
    \begin{center}
    $ Precision = \frac{relevant\ documents \cap retrieved\ documents}{retrieved\ documents} = \frac{|R_i \cap A_i|}{|A_i|} $ \\
    \end{center} 
    
    Recall : \\
    Results in a number between 0 and 1, Answering: how much of the total correct results was provided by the found result. Higher is better.
    \begin{center}
    $ Recall = \frac{relevant\ documents \cap retrieved\ documents}{relevant\ documents} = \frac{|R_i \cap A_i|}{|R_i|} $ \\
    \end{center} 
    
    F-score: \\
    Use the precision and the recall to determine the accuracy. It is a value between 0 and 1. Higher is better.
    \begin{center}
    $ Recall = \frac{2 \times \pi_i \times \rho_i}{\pi + \rho_i} $\\
    \end{center} 


\section{Results}
The result set is pretty large, therefore unfit to be displayed in this document. Listed below is the average per calculated factor. To see the full result set, please reference the source at \url{https://docs.google.com/spreadsheets/d/1NH385ixes9ILRXGzjTgCat6tBuEqB1W68g7rxCs6agQ/edit?usp=sharing}.
\subsection{Data}

\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 Real time factor & 0.07 \\ 
 Word error rate & 0.46 \\ 
 Word recognition rate & 0.54 \\ 
 Word correct rate & 0.57 \\ 
 Precision micro & 0.37 \\ 
 Recall micro & 0.37 \\ 
 F\_score micro & 0.37 \\  
 Precision macro & 0.40 \\  
 Recall macro & 0.34 \\  
 F\_score macro & 0.35 \\   
 \hline
\end{tabular}
\end{center}

\subsection{Remarkable}
    'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives. \\
    'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. \\ \\
    The average real time factor is 0.07. This means on average it takes 0.07 seconds to recognize a single character. The audio file "She's such a dear friend." took 2.95 seconds to recognize (0.118 RTF), this is way slower than we expected in the hypothesis. Although we see a correlation between long and short audio files - long audio files have a faster real time factor - we did not expect it to be this slow. An option to fasten up the proces is to make use of Azure's continuous flow. This rules out the need of making a new connection for every single file and thus should speed up the process. \\ \\
    The correctness is good, but not as good as expected in the hypothesis. This could be caused by the bad transcription of the test set. 
    Another cause could be some corrupt audio files in the test set. This can be seen in the number of words found in the transcription compared to the number of words found in the generated transcription. This difference is sometime 30 words in the transcript versus 6 in the generated transcript. More than half the audio is sometimes missing, which will of course change the average drastically. 
    
\section{Discussion}
The test set that was used was made by fellow students. This has the consequence that the quality of the tests varies within the test set. For example, one of the test sentences ends, according to the provided transcript, with 'in 619 AD'. However the algorithm recognized this as 'in 6 1980'. When we listened to the audio fragment ourselves, we also thought the fragment said 'in 1980'. This means that the quality of our result set may vary, because the used test set also varies in quality. \\ \\
Some audio files sent to the Azure text to speech service are only translated partially. Because of this, some audio files seem to be translated very poorly, but this is not always the case. We have not been able to locate the reason for this issue. According to microsoft\cite{Accuracyt2s} a WER of ~20\% is acceptable, however our WER at this very moment is 46\%. The main reason for this is files being translated only partially.

\section{Conclusion}


\bibliographystyle{acm} 
\bibliography{References} 

\end{document}
